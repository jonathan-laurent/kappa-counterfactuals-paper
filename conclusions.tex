% -*- TeX-master: "ijcai18.tex" -*-

\section*{Conclusions and Future Work}

In this paper, we propose a method for giving meaning to
counterfactual statements in mechanistic models of
complex physical processes.  We modify the continuous-time Monte Carlo
algorithm that is used to generate traces in these models so as to
sample counterfactual trajectories that stay probabilistically as
close to the original (factual) trajectory as an intervention permits
them to be. We then construct causal diagrams that explain
counterfactual dependencies in terms of enablement and prevention
relations between events. Enablement is a standard causal relation
between events within factual or within counterfactual traces, whereas
prevention shows up as a relation between pairs of events, one in the
factual and the other in the counterfactual trace. In particular,
prevention can involve events that were not observed in the factual
trace. This results in explanatory diagrams that resonate more with
the ubiquitous presence of inhibitory interactions in biology and that
can capture subtle kinetic aspects of rule-based models.  Our
completeness result, according to which any counterfactual dependency
can be ``explained'' in terms of enablement and prevention, increases
our confidence in this approach to counterfactuals and connects two
visions of causality that are typically treated disjointly.

Despite a sound theoretical foundation and an effective
implementation, our technique is in need of rigorous practical
assessment using large-scale models. Besides, difficult practical
questions remain. Most notably, which counterfactual experiments are
worth trying? It is unclear a priori which interventions are
informative for traces that include many millions of events. At
present, we can only offer tentative directions for future study.

One way to identify interventions worth making without relying on
expert knowledge is by developing heuristics, such as recognizing
correlations between events in samples of factual traces (or in a
single long trace). In our toy model, the occurrence of $p$ is often
preceded by the occurrence of $pk$. This correlation, together with
the absence of $pk$ from some initial causal account---such as the one
presently achievable in a fully automated fashion based on enablement
alone, (Figure~\ref{fig:dumb-story})---suggests to try a
counterfactual experiment on $pk$. More generally,
if \begin{inparaenum}[(i)] \item a context $\mathcal C$ in which an
  event $e$ occurs is frequently more specific than is required by the
  left-hand side of the underlying rule and \item this observation
  cannot be explained by the current causal
  narrative, \end{inparaenum} then a counterfactual experiment in
which we block the last event responsible for at least part of
$\mathcal C$ seems worthwhile in order to assess whether the current
causal narrative needs to be updated.  Another important practical
question is whether interventions should block single events or
``knock out'' related event templates for a defined timeframe. Our
framework accomodates both approaches, as exemplified in
Appendix~\ref{ap:benchmark}.

On a more conceptual side, we are investigating principled ways of
``gluing'' together all explanatory accounts (such as
Figure~\ref{fig:cex}) that correspond to different counterfactual
experiments. This would summarize the causal structure of a system
relative to an outcome of interest. One wonders whether such a summary
diagram might constitute a basis for obtaining approximate structural
equations for complex mechanistic models. Replacing a rule-based model
with such equations could enable targeted statistical analysis to
estimate model parameters, for which simulations would be too
expensive.

\ifreview \ifincludeappendices \else
\paragraph{Note to the reviewers}
A version of this paper with appendices containing the proofs of
Proposition~\ref{prop:div-activity} and Theorem~\ref{thm:completeness}
along with a benchmark of our implementation of counterfactual
resimulation on a scaled-up version of our example model can be found
following this
\underline{\href{https://www.dropbox.com/sh/2fwji0its0o0ciq/AABfLZ-GO2wCE2x3h3ulUbB-a?dl=0}{link}}
(anonymous).
%\url{http://www.lamsade.dauphine.fr/~lang/IJCAI-ECAI-2018/FAQ-authors.html}
\fi
\fi